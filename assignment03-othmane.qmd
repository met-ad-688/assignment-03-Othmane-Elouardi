---
title: "Assignment 3"
subtitle: "Big Data Visualization on Scale"
author:
  - name: "Othmane Elouardi"
    affiliation: "Boston University"
date: today
number-sections: true
format:
  html:
    theme: [lux, styles.scss]
    toc: true
    toc-depth: 3
    code-fold: false       # hide fold UI
    code-tools: false      # hide copy/run UI
    code-copy: false
    highlight-style: github
    title-block-style: default
    title-block-banner: true
    df-print: paged
execute:
  enabled: true            # <-- must be in the header
  echo: false
  warning: false
  message: false
  freeze: auto
---

---


# Load the Dataset

&emsp;&emsp;I used **PySpark** to bring the `lightcast_data.csv` file into a Spark DataFrame. After loading the data, I checked the schema to confirm that the column names were correct and could be used reliably in the later analysis. For each visualization, I customized the color schemes, fonts, and styles rather than relying on defaults, and I added a short explanation under each figure to highlight the main insights.  

---

## Loading Dataset With PySpark

&emsp;• Initialized a Spark session  
&emsp;• Loaded the `lightcast_data.csv` file into a DataFrame  
&emsp;• Displayed the schema and a sample of the dataset as a quick check  



```python
# Imports
import pandas as pd
import plotly.express as px
import plotly.io as pio
from pyspark.sql import SparkSession
import re
import numpy as np
import plotly.graph_objects as go
from pyspark.sql.functions import col, split, explode, regexp_replace, transform, when
from pyspark.sql import functions as F
from pyspark.sql.functions import col, monotonically_increasing_id
```

```python
# Spark session
spark = (
    SparkSession.builder
    .appName("LightcastData")
    .getOrCreate()
)

# Load the CSV
csv_path = "data/lightcast_job_postings.csv"

df = (
    spark.read
        .option("header", "true")
        .option("inferSchema", "true")
        .option("multiLine", "true")
        .option("escape", "\"")
        .csv(csv_path)
)

# Temp view for SQL
df.createOrReplaceTempView("job_postings")

# Quick diagnostics 
#print("\n--- Spark & Data Quick Check ---")
#print("Spark version:", spark.version)
#print("Row count (sampled below):")
#df.show(5, truncate=False)    
# df.printSchema()            
```



## DATA CLEANING

In this section, we performed several cleaning steps to ensure the dataset is consistent and ready for analysis:

- Cast salary/experience columns to numeric  
- Compute medians (via `approxQuantile`) for imputation  
- Create `Average_Salary` with sensible fallbacks  
- Clean `EDUCATION_LEVELS_NAME` (remove newlines)  
- Derive a simplified `REMOTE_GROUP` (`Remote`, `Hybrid`, `Onsite`)  

---

### Casting

```python
# Cast target columns to numeric (DoubleType)
from pyspark.sql.types import DoubleType

numeric_casts = {
    "SALARY_FROM": DoubleType(),
    "SALARY_TO":   DoubleType(),
    "SALARY":      DoubleType(),
    "MIN_YEARS_EXPERIENCE": DoubleType(),
    "MAX_YEARS_EXPERIENCE": DoubleType()
}

df = df.select(*[
    F.col(c).cast(numeric_casts[c]).alias(c) if c in numeric_casts else F.col(c)
    for c in df.columns
])
```

### Medians

```python
# Helper to compute median with approxQuantile
def median_of(colname):
    vals = df.filter(F.col(colname).isNotNull()) \
             .approxQuantile(colname, [0.5], 0.001)
    return float(vals[0]) if vals else None

median_from   = median_of("SALARY_FROM")
median_to     = median_of("SALARY_TO")
median_salary = median_of("SALARY")

print("Medians:", median_from, median_to, median_salary)
```














### Average Salary

```python
# Choose a global fallback for missing salary values
fallback = next(x for x in [median_salary, median_from, median_to] if x is not None)

df = df.withColumn(
    "Average_Salary",
    F.when(F.col("SALARY_FROM").isNotNull() & F.col("SALARY_TO").isNotNull(),
           (F.col("SALARY_FROM") + F.col("SALARY_TO")) / 2.0
    ).when(F.col("SALARY").isNotNull(), F.col("SALARY")
    ).when(F.col("SALARY_FROM").isNotNull(), F.col("SALARY_FROM")
    ).when(F.col("SALARY_TO").isNotNull(), F.col("SALARY_TO")
    ).otherwise(F.lit(fallback))
)

# fill missing bounds for completeness
df = df.fillna({"SALARY_FROM": median_from, "SALARY_TO": median_to})
```


### Clean Education Field

```python
# Remove \r and \n, then trim
df = df.withColumn(
    "EDUCATION_LEVELS_NAME",
    F.trim(
        F.regexp_replace(
            F.regexp_replace(F.col("EDUCATION_LEVELS_NAME"), r"\r", " "),
            r"\n", " "
        )
    )
)
```

### Remote Group

```python
# Normalize to Remote / Hybrid / Onsite
df = df.withColumn(
    "REMOTE_GROUP",
    F.when(F.col("REMOTE_TYPE_NAME") == "Remote", "Remote")
     .when(F.col("REMOTE_TYPE_NAME") == "Hybrid", "Hybrid")
     .otherwise("Onsite")
)
```

### Final Check

```python
df.createOrReplaceTempView("job_postings_clean")

print("Rows retained:", df.count())

df.select("Average_Salary","SALARY","EDUCATION_LEVELS_NAME",
          "REMOTE_TYPE_NAME","REMOTE_GROUP",
          "MIN_YEARS_EXPERIENCE","MAX_YEARS_EXPERIENCE") \
  .show(5, truncate=False)
```











# Salary Distribution by Industry and Employment Type

In this section, we explore salary variation across industries and employment types:

- **Filter the dataset**
  - Remove records where **salary is missing or zero**.

- **Aggregate / organize**
  - Group by **NAICS industry** using `NAICS2_NAME`.
  - Keep salary values from `SALARY_FROM` (or `Average_Salary` if preferred).
  - Separate distributions by **employment type** using `EMPLOYMENT_TYPE_NAME`.

- **Visualize results**
  - Create a **box plot** where:
    - **X-axis** = `NAICS2_NAME`
    - **Y-axis** = `SALARY_FROM`
    - **Color / group** = `EMPLOYMENT_TYPE_NAME`
  - Customize axis labels, tick angle, margins, and figure size for readability.





```{python}
#| echo: false
#| warning: false
#| message: false
#| fig-cap: "Salary Distribution by Industry and Employment Type"

from pyspark.sql import SparkSession, functions as F
from pyspark.sql.types import DoubleType
from pyspark.sql.functions import col
import plotly.express as px
from IPython.display import HTML, display

# Reuse Spark session if already started, otherwise create new
try:
    spark
except NameError:
    spark = SparkSession.builder.appName("LightcastData").getOrCreate()

# Reuse cleaned table if exists, else reload CSV
if any(t.name == "job_postings_clean" for t in spark.catalog.listTables()):
    df_use = spark.table("job_postings_clean")
else:
    df_use = (
        spark.read
        .option("header", "true")
        .option("inferSchema", "true")
        .option("multiLine", "true")
        .option("escape", "\"")
        .csv("data/lightcast_job_postings.csv")
        .withColumn("SALARY_FROM", F.col("SALARY_FROM").cast(DoubleType()))
    )

# Filter and select relevant columns
plot_sdf = (
    df_use.filter((col("SALARY_FROM").isNotNull()) & (col("SALARY_FROM") > 0))
          .select("NAICS2_NAME", "SALARY_FROM", "EMPLOYMENT_TYPE_NAME")
)

# Convert to pandas for Plotly
plot_df = plot_sdf.toPandas()
plot_df["NAICS2_NAME"] = plot_df["NAICS2_NAME"].astype(str).str.replace(r"\s+", " ", regex=True)

# Build Plotly box plot
fig = px.box(
    plot_df,
    x="NAICS2_NAME",
    y="SALARY_FROM",
    color="EMPLOYMENT_TYPE_NAME",
    points="outliers",
    labels={
        "NAICS2_NAME": "Industry (NAICS 2-Digit)",
        "SALARY_FROM": "Salary",
        "EMPLOYMENT_TYPE_NAME": "Employment Type",
    },
    title="Salary Distribution by Industry and Employment Type",
)

# Make figure larger and more readable
fig.update_layout(
    xaxis_title=None,
    yaxis_title="Salary",
    margin=dict(l=40, r=40, t=80, b=350),  # more bottom margin
    boxmode="group",
    width=2000,   # much wider
    height=800    # taller
)
fig.update_xaxes(tickangle=45)


# Render as HTML so it shows on Quarto site
display(HTML(fig.to_html(include_plotlyjs="cdn", full_html=False)))
```



- **Explanation**:
  “ This box plot shows salary variations by industry and employment type. Full-time positions tend to have higher median salaries and greater variability compared to part-time roles. Outliers, particularly in Construction, Health Care, and Finance, highlight some extremely high or low salaries, providing a clear view of the salary landscape across sectors.”
