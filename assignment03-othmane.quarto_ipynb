{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Assignment 3\"\n",
        "subtitle: \"Big Data Visualization on Scale\"\n",
        "author:\n",
        "  - name: \"Othmane Elouardi\"\n",
        "    affiliation: \"Boston University\"\n",
        "date: today\n",
        "number-sections: true\n",
        "format:\n",
        "  html:\n",
        "    theme: [lux, styles.scss]\n",
        "    toc: true\n",
        "    toc-depth: 3\n",
        "    code-fold: false       # hide fold UI\n",
        "    code-tools: false      # hide copy/run UI\n",
        "    code-copy: false\n",
        "    highlight-style: github\n",
        "    title-block-style: default\n",
        "    title-block-banner: true\n",
        "    df-print: paged\n",
        "execute:\n",
        "  enabled: true            # <-- must be in the header\n",
        "  echo: false\n",
        "  warning: false\n",
        "  message: false\n",
        "  freeze: auto\n",
        "---\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# Load the Dataset\n",
        "\n",
        "&emsp;&emsp;I used **PySpark** to bring the `lightcast_data.csv` file into a Spark DataFrame. After loading the data, I checked the schema to confirm that the column names were correct and could be used reliably in the later analysis. For each visualization, I customized the color schemes, fonts, and styles rather than relying on defaults, and I added a short explanation under each figure to highlight the main insights.  \n",
        "\n",
        "---\n",
        "\n",
        "## Loading Dataset With PySpark\n",
        "\n",
        "&emsp;• Initialized a Spark session  \n",
        "&emsp;• Loaded the `lightcast_data.csv` file into a DataFrame  \n",
        "&emsp;• Displayed the schema and a sample of the dataset as a quick check  \n",
        "\n",
        "\n",
        "\n",
        "```python\n",
        "# Imports\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "import plotly.io as pio\n",
        "from pyspark.sql import SparkSession\n",
        "import re\n",
        "import numpy as np\n",
        "import plotly.graph_objects as go\n",
        "from pyspark.sql.functions import col, split, explode, regexp_replace, transform, when\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.functions import col, monotonically_increasing_id\n",
        "```\n",
        "\n",
        "```python\n",
        "# Spark session\n",
        "spark = (\n",
        "    SparkSession.builder\n",
        "    .appName(\"LightcastData\")\n",
        "    .getOrCreate()\n",
        ")\n",
        "\n",
        "# Load the CSV\n",
        "csv_path = \"data/lightcast_job_postings.csv\"\n",
        "\n",
        "df = (\n",
        "    spark.read\n",
        "        .option(\"header\", \"true\")\n",
        "        .option(\"inferSchema\", \"true\")\n",
        "        .option(\"multiLine\", \"true\")\n",
        "        .option(\"escape\", \"\\\"\")\n",
        "        .csv(csv_path)\n",
        ")\n",
        "\n",
        "# Temp view for SQL\n",
        "df.createOrReplaceTempView(\"job_postings\")\n",
        "\n",
        "# Quick diagnostics \n",
        "#print(\"\\n--- Spark & Data Quick Check ---\")\n",
        "#print(\"Spark version:\", spark.version)\n",
        "#print(\"Row count (sampled below):\")\n",
        "#df.show(5, truncate=False)    \n",
        "# df.printSchema()            \n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "## DATA CLEANING\n",
        "\n",
        "In this section, we performed several cleaning steps to ensure the dataset is consistent and ready for analysis:\n",
        "\n",
        "- Cast salary/experience columns to numeric  \n",
        "- Compute medians (via `approxQuantile`) for imputation  \n",
        "- Create `Average_Salary` with sensible fallbacks  \n",
        "- Clean `EDUCATION_LEVELS_NAME` (remove newlines)  \n",
        "- Derive a simplified `REMOTE_GROUP` (`Remote`, `Hybrid`, `Onsite`)  \n",
        "\n",
        "---\n",
        "\n",
        "### Casting\n",
        "\n",
        "```python\n",
        "# Cast target columns to numeric (DoubleType)\n",
        "from pyspark.sql.types import DoubleType\n",
        "\n",
        "numeric_casts = {\n",
        "    \"SALARY_FROM\": DoubleType(),\n",
        "    \"SALARY_TO\":   DoubleType(),\n",
        "    \"SALARY\":      DoubleType(),\n",
        "    \"MIN_YEARS_EXPERIENCE\": DoubleType(),\n",
        "    \"MAX_YEARS_EXPERIENCE\": DoubleType()\n",
        "}\n",
        "\n",
        "df = df.select(*[\n",
        "    F.col(c).cast(numeric_casts[c]).alias(c) if c in numeric_casts else F.col(c)\n",
        "    for c in df.columns\n",
        "])\n",
        "```\n",
        "\n",
        "### Medians\n",
        "\n",
        "```python\n",
        "# Helper to compute median with approxQuantile\n",
        "def median_of(colname):\n",
        "    vals = df.filter(F.col(colname).isNotNull()) \\\n",
        "             .approxQuantile(colname, [0.5], 0.001)\n",
        "    return float(vals[0]) if vals else None\n",
        "\n",
        "median_from   = median_of(\"SALARY_FROM\")\n",
        "median_to     = median_of(\"SALARY_TO\")\n",
        "median_salary = median_of(\"SALARY\")\n",
        "\n",
        "print(\"Medians:\", median_from, median_to, median_salary)\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#### Medians (output)"
      ],
      "id": "2174d7b9"
    },
    {
      "cell_type": "code",
      "metadata": {
        "message": false
      },
      "source": [
        "#| echo: false\n",
        "#| warning: false\n",
        "\n",
        "from pyspark.sql import SparkSession, functions as F\n",
        "\n",
        "# Ensure Spark & a DataFrame named `df` are available\n",
        "try:\n",
        "    spark\n",
        "except NameError:\n",
        "    spark = SparkSession.builder.appName(\"LightcastData\").getOrCreate()\n",
        "\n",
        "try:\n",
        "    df  # reuse if already defined earlier\n",
        "except NameError:\n",
        "    if any(t.name == \"job_postings_clean\" for t in spark.catalog.listTables()):\n",
        "        df = spark.table(\"job_postings_clean\")\n",
        "    else:\n",
        "        df = (\n",
        "            spark.read\n",
        "            .option(\"header\", \"true\").option(\"inferSchema\", \"true\")\n",
        "            .option(\"multiLine\", \"true\").option(\"escape\", \"\\\"\")\n",
        "            .csv(\"data/lightcast_job_postings.csv\")\n",
        "        )\n",
        "\n",
        "# Helper to compute median with approxQuantile (returns None if column missing)\n",
        "def median_of(colname):\n",
        "    if colname not in df.columns:\n",
        "        return None\n",
        "    vals = df.filter(F.col(colname).isNotNull()).approxQuantile(colname, [0.5], 0.001)\n",
        "    return float(vals[0]) if vals else None\n",
        "\n",
        "# Compute medians\n",
        "median_from   = median_of(\"SALARY_FROM\")\n",
        "median_to     = median_of(\"SALARY_TO\")\n",
        "median_salary = median_of(\"SALARY\")\n",
        "\n",
        "# Nicely formatted output (only text appears on page)\n",
        "from IPython.display import display, HTML\n",
        "html = f\"\"\"\n",
        "<div style=\"font-size:1rem; line-height:1.6;\">\n",
        "  <strong>Medians</strong>\n",
        "  <ul style=\"margin:0.25rem 0 0 1.25rem;\">\n",
        "    <li>SALARY_FROM: {median_from:,.0f}</li>\n",
        "    <li>SALARY_TO: {median_to:,.0f}</li>\n",
        "    <li>SALARY: {median_salary:,.0f}</li>\n",
        "  </ul>\n",
        "</div>\n",
        "\"\"\"\n",
        "display(HTML(html))"
      ],
      "id": "b4d4ffdb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Average Salary\n",
        "\n",
        "```python\n",
        "# Choose a global fallback for missing salary values\n",
        "fallback = next(x for x in [median_salary, median_from, median_to] if x is not None)\n",
        "\n",
        "df = df.withColumn(\n",
        "    \"Average_Salary\",\n",
        "    F.when(F.col(\"SALARY_FROM\").isNotNull() & F.col(\"SALARY_TO\").isNotNull(),\n",
        "           (F.col(\"SALARY_FROM\") + F.col(\"SALARY_TO\")) / 2.0\n",
        "    ).when(F.col(\"SALARY\").isNotNull(), F.col(\"SALARY\")\n",
        "    ).when(F.col(\"SALARY_FROM\").isNotNull(), F.col(\"SALARY_FROM\")\n",
        "    ).when(F.col(\"SALARY_TO\").isNotNull(), F.col(\"SALARY_TO\")\n",
        "    ).otherwise(F.lit(fallback))\n",
        ")\n",
        "\n",
        "# fill missing bounds for completeness\n",
        "df = df.fillna({\"SALARY_FROM\": median_from, \"SALARY_TO\": median_to})\n",
        "```\n",
        "\n",
        "\n",
        "#### Average Salary (output)\n"
      ],
      "id": "3619b9c1"
    },
    {
      "cell_type": "code",
      "metadata": {
        "message": false
      },
      "source": [
        "#| echo: false\n",
        "#| warning: false\n",
        "\n",
        "from pyspark.sql import SparkSession, functions as F\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "# Ensure Spark & df exist\n",
        "try:\n",
        "    spark\n",
        "except NameError:\n",
        "    spark = SparkSession.builder.appName(\"LightcastData\").getOrCreate()\n",
        "\n",
        "try:\n",
        "    df  # reuse if defined earlier\n",
        "except NameError:\n",
        "    if any(t.name == \"job_postings_clean\" for t in spark.catalog.listTables()):\n",
        "        df = spark.table(\"job_postings_clean\")\n",
        "    else:\n",
        "        df = (\n",
        "            spark.read\n",
        "            .option(\"header\", \"true\").option(\"inferSchema\", \"true\")\n",
        "            .option(\"multiLine\", \"true\").option(\"escape\", \"\\\"\")\n",
        "            .csv(\"data/lightcast_job_postings.csv\")\n",
        "        )\n",
        "\n",
        "# Helper to compute median if we need to rebuild fallback\n",
        "def _median_of(colname):\n",
        "    if colname not in df.columns:\n",
        "        return None\n",
        "    vals = df.filter(F.col(colname).isNotNull()) \\\n",
        "             .approxQuantile(colname, [0.5], 0.001)\n",
        "    return float(vals[0]) if vals else None\n",
        "\n",
        "# Use existing `fallback` if present; otherwise rebuild it exactly like before\n",
        "try:\n",
        "    avg_value = float(fallback)\n",
        "except NameError:\n",
        "    median_from   = _median_of(\"SALARY_FROM\")\n",
        "    median_to     = _median_of(\"SALARY_TO\")\n",
        "    median_salary = _median_of(\"SALARY\")\n",
        "    # same selection rule as your pipeline\n",
        "    for x in (median_salary, median_from, median_to):\n",
        "        if x is not None:\n",
        "            avg_value = x\n",
        "            break\n",
        "    else:\n",
        "        avg_value = None  # if everything missing\n",
        "\n",
        "# Render result (text only)\n",
        "if avg_value is None:\n",
        "    display(HTML(\"<em>Average Salary could not be determined from the available fields.</em>\"))\n",
        "else:\n",
        "    display(HTML(f\"\"\"\n",
        "    <div style=\"font-size:1rem; line-height:1.6;\">\n",
        "      <strong>Average Salary</strong>: {avg_value:,.0f} USD\n",
        "    </div>\n",
        "    \"\"\"))"
      ],
      "id": "77b57f6e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Clean Education Field\n",
        "\n",
        "```python\n",
        "# Remove \\r and \\n, then trim\n",
        "df = df.withColumn(\n",
        "    \"EDUCATION_LEVELS_NAME\",\n",
        "    F.trim(\n",
        "        F.regexp_replace(\n",
        "            F.regexp_replace(F.col(\"EDUCATION_LEVELS_NAME\"), r\"\\r\", \" \"),\n",
        "            r\"\\n\", \" \"\n",
        "        )\n",
        "    )\n",
        ")\n",
        "```\n",
        "\n",
        "\n",
        "#### Clean Education Field (output)"
      ],
      "id": "ff0882b4"
    },
    {
      "cell_type": "code",
      "metadata": {
        "message": false
      },
      "source": [
        "#| echo: false\n",
        "#| warning: false\n",
        "\n",
        "from pyspark.sql import SparkSession, functions as F\n",
        "from pyspark.sql.functions import col\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "# Ensure Spark & df exist\n",
        "try:\n",
        "    spark\n",
        "except NameError:\n",
        "    spark = SparkSession.builder.appName(\"LightcastData\").getOrCreate()\n",
        "\n",
        "try:\n",
        "    df\n",
        "except NameError:\n",
        "    if any(t.name == \"job_postings_clean\" for t in spark.catalog.listTables()):\n",
        "        df = spark.table(\"job_postings_clean\")\n",
        "    else:\n",
        "        df = (\n",
        "            spark.read\n",
        "            .option(\"header\", \"true\").option(\"inferSchema\", \"true\")\n",
        "            .option(\"multiLine\", \"true\").option(\"escape\", \"\\\"\")\n",
        "            .csv(\"data/lightcast_job_postings.csv\")\n",
        "        )\n",
        "\n",
        "colname = \"EDUCATION_LEVELS_NAME\"\n",
        "\n",
        "if colname not in df.columns:\n",
        "    display(HTML(\"<em>EDUCATION_LEVELS_NAME column not found — nothing to summarize.</em>\"))\n",
        "else:\n",
        "    total_rows = df.count()\n",
        "    # blanks or nulls *after* cleaning step\n",
        "    blanks = (\n",
        "        df.filter( (col(colname).isNull()) | (F.trim(col(colname)) == \"\") )\n",
        "          .count()\n",
        "    )\n",
        "\n",
        "    # Top 10 education labels\n",
        "    top_pdf = (\n",
        "        df.groupBy(colname).count()\n",
        "          .orderBy(F.desc(\"count\"))\n",
        "          .limit(10)\n",
        "          .toPandas()\n",
        "    )\n",
        "\n",
        "    # Build HTML\n",
        "    header = f\"\"\"\n",
        "    <div style=\"font-size:1rem; line-height:1.6; margin-bottom:.5rem;\">\n",
        "      <strong>Education Field Summary (post-clean)</strong><br/>\n",
        "      Total rows: {total_rows:,} • Blank/Null: {blanks:,}\n",
        "    </div>\n",
        "    \"\"\"\n",
        "\n",
        "    # Simple table\n",
        "    if len(top_pdf) > 0:\n",
        "        # pretty format\n",
        "        top_pdf.columns = [\"Education Level\", \"Count\"]\n",
        "        top_pdf[\"Count\"] = top_pdf[\"Count\"].map(lambda x: f\"{x:,}\")\n",
        "        table_html = top_pdf.to_html(index=False, border=0)\n",
        "        display(HTML(header + table_html))\n",
        "    else:\n",
        "        display(HTML(header + \"<em>No non-empty education values found.</em>\"))"
      ],
      "id": "3009e23d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Remote Group\n",
        "\n",
        "```python\n",
        "# Normalize to Remote / Hybrid / Onsite\n",
        "df = df.withColumn(\n",
        "    \"REMOTE_GROUP\",\n",
        "    F.when(F.col(\"REMOTE_TYPE_NAME\") == \"Remote\", \"Remote\")\n",
        "     .when(F.col(\"REMOTE_TYPE_NAME\") == \"Hybrid\", \"Hybrid\")\n",
        "     .otherwise(\"Onsite\")\n",
        ")\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### Final Check\n",
        "\n",
        "```python\n",
        "df.createOrReplaceTempView(\"job_postings_clean\")\n",
        "\n",
        "print(\"Rows retained:\", df.count())\n",
        "\n",
        "df.select(\"Average_Salary\",\"SALARY\",\"EDUCATION_LEVELS_NAME\",\n",
        "          \"REMOTE_TYPE_NAME\",\"REMOTE_GROUP\",\n",
        "          \"MIN_YEARS_EXPERIENCE\",\"MAX_YEARS_EXPERIENCE\") \\\n",
        "  .show(5, truncate=False)\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Salary Distribution by Industry and Employment Type\n",
        "\n",
        "In this section, we explore salary variation across industries and employment types:\n",
        "\n",
        "- **Filter the dataset**\n",
        "  - Remove records where **salary is missing or zero**.\n",
        "\n",
        "- **Aggregate / organize**\n",
        "  - Group by **NAICS industry** using `NAICS2_NAME`.\n",
        "  - Keep salary values from `SALARY_FROM` (or `Average_Salary` if preferred).\n",
        "  - Separate distributions by **employment type** using `EMPLOYMENT_TYPE_NAME`.\n",
        "\n",
        "- **Visualize results**\n",
        "  - Create a **box plot** where:\n",
        "    - **X-axis** = `NAICS2_NAME`\n",
        "    - **Y-axis** = `SALARY_FROM`\n",
        "    - **Color / group** = `EMPLOYMENT_TYPE_NAME`\n",
        "  - Customize axis labels, tick angle, margins, and figure size for readability.\n"
      ],
      "id": "fbc8479c"
    },
    {
      "cell_type": "code",
      "metadata": {
        "message": false
      },
      "source": [
        "#| echo: false\n",
        "#| warning: false\n",
        "#| fig-cap: Salary Distribution by Industry and Employment Type\n",
        "\n",
        "from pyspark.sql import SparkSession, functions as F\n",
        "from pyspark.sql.types import DoubleType\n",
        "from pyspark.sql.functions import col\n",
        "import plotly.express as px\n",
        "from IPython.display import HTML, display\n",
        "\n",
        "# Reuse Spark session if already started, otherwise create new\n",
        "try:\n",
        "    spark\n",
        "except NameError:\n",
        "    spark = SparkSession.builder.appName(\"LightcastData\").getOrCreate()\n",
        "\n",
        "# Reuse cleaned table if exists, else reload CSV\n",
        "if any(t.name == \"job_postings_clean\" for t in spark.catalog.listTables()):\n",
        "    df_use = spark.table(\"job_postings_clean\")\n",
        "else:\n",
        "    df_use = (\n",
        "        spark.read\n",
        "        .option(\"header\", \"true\")\n",
        "        .option(\"inferSchema\", \"true\")\n",
        "        .option(\"multiLine\", \"true\")\n",
        "        .option(\"escape\", \"\\\"\")\n",
        "        .csv(\"data/lightcast_job_postings.csv\")\n",
        "        .withColumn(\"SALARY_FROM\", F.col(\"SALARY_FROM\").cast(DoubleType()))\n",
        "    )\n",
        "\n",
        "# Filter and select relevant columns\n",
        "plot_sdf = (\n",
        "    df_use.filter((col(\"SALARY_FROM\").isNotNull()) & (col(\"SALARY_FROM\") > 0))\n",
        "          .select(\"NAICS2_NAME\", \"SALARY_FROM\", \"EMPLOYMENT_TYPE_NAME\")\n",
        ")\n",
        "\n",
        "# Convert to pandas for Plotly\n",
        "plot_df = plot_sdf.toPandas()\n",
        "plot_df[\"NAICS2_NAME\"] = plot_df[\"NAICS2_NAME\"].astype(str).str.replace(r\"\\s+\", \" \", regex=True)\n",
        "\n",
        "# Build Plotly box plot\n",
        "fig = px.box(\n",
        "    plot_df,\n",
        "    x=\"NAICS2_NAME\",\n",
        "    y=\"SALARY_FROM\",\n",
        "    color=\"EMPLOYMENT_TYPE_NAME\",\n",
        "    points=\"outliers\",\n",
        "    labels={\n",
        "        \"NAICS2_NAME\": \"Industry (NAICS 2-Digit)\",\n",
        "        \"SALARY_FROM\": \"Salary\",\n",
        "        \"EMPLOYMENT_TYPE_NAME\": \"Employment Type\",\n",
        "    },\n",
        "    title=\"Salary Distribution by Industry and Employment Type\",\n",
        ")\n",
        "\n",
        "# Make figure larger and more readable\n",
        "fig.update_layout(\n",
        "    xaxis_title=None,\n",
        "    yaxis_title=\"Salary\",\n",
        "    margin=dict(l=40, r=40, t=80, b=350),  # more bottom margin\n",
        "    boxmode=\"group\",\n",
        "    width=2000,   # much wider\n",
        "    height=800    # taller\n",
        ")\n",
        "fig.update_xaxes(tickangle=45)\n",
        "\n",
        "\n",
        "# Render as HTML so it shows on Quarto site\n",
        "display(HTML(fig.to_html(include_plotlyjs=\"cdn\", full_html=False)))"
      ],
      "id": "b9a8c290",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- **Explanation**:\n",
        "  “ This box plot shows salary variations by industry and employment type. Full-time positions tend to have higher median salaries and greater variability compared to part-time roles. Outliers, particularly in Construction, Health Care, and Finance, highlight some extremely high or low salaries, providing a clear view of the salary landscape across sectors.”\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Salary Analysis by ONET Occupation Type (Bubble Chart)\n",
        "\n",
        "- Analyze how salaries differ across ONET occupation types.  \n",
        "- **Aggregate Data**  \n",
        "  - Compute median salary for each occupation in the ONET taxonomy.  \n",
        "- **Visualize results**  \n",
        "  - Create a **bubble chart** where:  \n",
        "    - X-axis = `ONET_NAME` (occupation)  \n",
        "    - Y-axis = `Median_Salary`  \n",
        "    - Bubble size = `Job_Postings` (number of postings)  \n",
        "  - Apply custom colors and font styles.  \n"
      ],
      "id": "08deb2a0"
    },
    {
      "cell_type": "code",
      "metadata": {
        "message": false
      },
      "source": [
        "#| echo: false\n",
        "#| warning: false\n",
        "#| fig-cap: Salary Analysis by Occupation (Auto-selected) – Bubble Chart\n",
        "\n",
        "from pyspark.sql import SparkSession, functions as F\n",
        "from pyspark.sql.types import DoubleType\n",
        "from pyspark.sql.functions import col\n",
        "import plotly.express as px\n",
        "from IPython.display import HTML, display\n",
        "import re\n",
        "\n",
        "# ---- Spark / source df\n",
        "try:\n",
        "    spark\n",
        "except NameError:\n",
        "    spark = SparkSession.builder.appName(\"LightcastData\").getOrCreate()\n",
        "\n",
        "if any(t.name == \"job_postings_clean\" for t in spark.catalog.listTables()):\n",
        "    base = spark.table(\"job_postings_clean\")\n",
        "else:\n",
        "    base = (\n",
        "        spark.read\n",
        "        .option(\"header\", \"true\").option(\"inferSchema\", \"true\")\n",
        "        .option(\"multiLine\", \"true\").option(\"escape\", \"\\\"\")\n",
        "        .csv(\"data/lightcast_job_postings.csv\")\n",
        "    )\n",
        "\n",
        "# ---- Choose salary column\n",
        "salary_col = \"Average_Salary\" if \"Average_Salary\" in base.columns else (\n",
        "    \"SALARY_FROM\" if \"SALARY_FROM\" in base.columns else \"SALARY\"\n",
        ")\n",
        "df = base.withColumn(salary_col, col(salary_col).cast(DoubleType()))\n",
        "\n",
        "# ---- Find occupation-like columns and pick the best by distinct count\n",
        "pattern = re.compile(r\"(ONET|OCCUP|OCC|TITLE).*\", re.IGNORECASE)\n",
        "occ_candidates = [c for c in df.columns if pattern.match(c)]\n",
        "# also include a few common fallbacks explicitly\n",
        "for c in [\"JOB_TITLE_NAME\", \"JOB_TITLE\", \"OCCUPATION_NAME\", \"OCC_TITLE\", \"ONET_NAME\"]:\n",
        "    if c in df.columns and c not in occ_candidates:\n",
        "        occ_candidates.append(c)\n",
        "\n",
        "distinct_counts = []\n",
        "for c in occ_candidates:\n",
        "    # count non-empty distinct values quickly\n",
        "    cnt = (df.where(F.col(c).isNotNull() & (F.length(F.col(c)) > 0))\n",
        "             .agg(F.approx_count_distinct(c).alias(\"n\")).collect()[0][\"n\"])\n",
        "    distinct_counts.append((c, cnt))\n",
        "\n",
        "distinct_counts.sort(key=lambda x: x[1], reverse=True)\n",
        "# pick the first with at least 5 distinct values; else fall back to the top one\n",
        "occ_col, occ_distinct = (distinct_counts[0] if distinct_counts else (None, 0))\n",
        "for c, n in distinct_counts:\n",
        "    if n >= 5:\n",
        "        occ_col, occ_distinct = c, n\n",
        "        break\n",
        "\n",
        "if not occ_col or occ_distinct <= 1:\n",
        "    raise ValueError(\n",
        "        f\"No suitable occupation column with variety was found. \"\n",
        "        f\"Checked: {distinct_counts}. Your dataset appears to have a single ONET occupation.\"\n",
        "    )\n",
        "\n",
        "# ---- Clean and explode multi-valued cells (comma/semicolon/pipe)\n",
        "clean = df.withColumn(occ_col, F.trim(F.regexp_replace(col(occ_col), r\"\\s+\", \" \")))\n",
        "split_col = F.split(F.regexp_replace(col(occ_col), r\"\\s*[,;/|]\\s*\", \"|\"), r\"\\|\")\n",
        "clean = clean.withColumn(\"Occupation\", F.explode(split_col))\n",
        "clean = clean.filter(F.col(\"Occupation\").isNotNull() & (F.length(F.col(\"Occupation\")) > 0))\n",
        "\n",
        "# ---- Aggregate\n",
        "agg = (\n",
        "    clean.filter(col(salary_col).isNotNull() & (col(salary_col) > 0))\n",
        "         .groupBy(\"Occupation\")\n",
        "         .agg(\n",
        "             F.percentile_approx(col(salary_col), 0.5, 10000).alias(\"Median_Salary\"),\n",
        "             F.count(F.lit(1)).alias(\"Job_Postings\")\n",
        "         )\n",
        ")\n",
        "\n",
        "TOP_N = 20\n",
        "pdf = (agg.orderBy(F.col(\"Job_Postings\").desc()).limit(TOP_N)).toPandas()\n",
        "pdf = pdf.sort_values(\"Job_Postings\", ascending=False)\n",
        "\n",
        "# ---- Plot\n",
        "fig = px.scatter(\n",
        "    pdf, x=\"Occupation\", y=\"Median_Salary\",\n",
        "    size=\"Job_Postings\", color=\"Job_Postings\",\n",
        "    color_continuous_scale=\"Viridis\", size_max=80,\n",
        "    hover_name=\"Occupation\",\n",
        "    labels={\n",
        "        \"Occupation\": \"Occupation Name\",\n",
        "        \"Median_Salary\": \"Median Salary\",\n",
        "        \"Job_Postings\": \"Number of Job Postings\"\n",
        "    },\n",
        "    title=f\"Salary Analysis by Occupation (Auto: {occ_col}, {occ_distinct} distinct)\"\n",
        ")\n",
        "fig.update_layout(width=1800, height=800, margin=dict(l=40, r=40, t=90, b=320))\n",
        "fig.update_xaxes(tickangle=45)\n",
        "display(HTML(fig.to_html(include_plotlyjs=\"cdn\", full_html=False)))"
      ],
      "id": "d1c624bc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- **Explanation**: \"This bubble chart illustrates the variation in median salaries across different occupations. The size of each bubble represents the number of job postings available for that role. What stands out from the data is that some positions, like Data Analyst, are in high demand, with a lot of job openings, but they offer moderate salaries. In contrast, more specialized roles, such as Data Engineer and Enterprise Architect, tend to have fewer job postings yet command much higher median salaries. This highlights an interesting trend in the job market where specialization can lead to greater financial rewards, even if the overall demand is lower.\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Salary by Education Level  \n",
        "\n",
        "- Create two groups:  \n",
        "  - **Associate’s or lower** (GED, Associate, No Education Listed)  \n",
        "  - **Bachelor’s** (Bachelor’s degree)  \n",
        "  - **Master’s** (Master’s degree)  \n",
        "  - **PhD** (Doctorate, professional degree)  \n",
        "- Plot scatter plots for each group using:  \n",
        "  - `MAX_YEARS_EXPERIENCE` (with jitter)  \n",
        "  - `Average_Salary`  \n",
        "  - `OCCUPATION_NAME` (labels optional for hover) "
      ],
      "id": "e230a6c4"
    },
    {
      "cell_type": "code",
      "metadata": {
        "message": false
      },
      "source": [
        "#| echo: false\n",
        "#| warning: false\n",
        "#| fig-cap: Experience vs Salary by Education Level\n",
        "\n",
        "from pyspark.sql import SparkSession, functions as F\n",
        "from pyspark.sql.types import DoubleType\n",
        "from pyspark.sql.functions import col\n",
        "from IPython.display import HTML, display\n",
        "import plotly.express as px\n",
        "import numpy as np\n",
        "\n",
        "# --- Spark / source df\n",
        "try:\n",
        "    spark\n",
        "except NameError:\n",
        "    spark = SparkSession.builder.appName(\"LightcastData\").getOrCreate()\n",
        "\n",
        "if any(t.name == \"job_postings_clean\" for t in spark.catalog.listTables()):\n",
        "    df = spark.table(\"job_postings_clean\")\n",
        "else:\n",
        "    df = (\n",
        "        spark.read\n",
        "        .option(\"header\", \"true\").option(\"inferSchema\", \"true\")\n",
        "        .option(\"multiLine\", \"true\").option(\"escape\", \"\\\"\")\n",
        "        .csv(\"data/lightcast_job_postings.csv\")\n",
        "    )\n",
        "\n",
        "# --- Pick experience column robustly\n",
        "exp_candidates = [\"MAX_YEARS_EXPERIENCE\", \"YEARS_EXPERIENCE\",\n",
        "                  \"YEARS_OF_EXPERIENCE\", \"MIN_YEARS_EXPERIENCE\"]\n",
        "exp_col = next((c for c in exp_candidates if c in df.columns), None)\n",
        "if exp_col is None:\n",
        "    raise ValueError(f\"No experience column found. Tried: {exp_candidates}\")\n",
        "\n",
        "# --- Pick salary column robustly (prefer your computed Average_Salary)\n",
        "if \"Average_Salary\" in df.columns:\n",
        "    sal_col = \"Average_Salary\"\n",
        "else:\n",
        "    if \"SALARY_FROM\" in df.columns and \"SALARY_TO\" in df.columns:\n",
        "        df = df.withColumn(\n",
        "            \"Average_Salary\",\n",
        "            (F.col(\"SALARY_FROM\").cast(DoubleType()) + F.col(\"SALARY_TO\").cast(DoubleType())) / 2.0\n",
        "        )\n",
        "        sal_col = \"Average_Salary\"\n",
        "    elif \"SALARY\" in df.columns:\n",
        "        sal_col = \"SALARY\"\n",
        "    else:\n",
        "        raise ValueError(\"No salary column found (looked for Average_Salary, SALARY_FROM/TO, SALARY).\")\n",
        "\n",
        "# Ensure salary is numeric\n",
        "df = df.withColumn(sal_col, col(sal_col).cast(DoubleType()))\n",
        "\n",
        "# --- Education grouping\n",
        "edu_src = \"EDUCATION_LEVELS_NAME\" if \"EDUCATION_LEVELS_NAME\" in df.columns else None\n",
        "if edu_src is None:\n",
        "    raise ValueError(\"EDUCATION_LEVELS_NAME column not found.\")\n",
        "\n",
        "df = df.withColumn(\n",
        "    \"EDU_GROUP\",\n",
        "    F.when(F.lower(F.col(edu_src)).contains(\"phd\"), \"PhD\")\n",
        "     .when(F.lower(F.col(edu_src)).contains(\"master\"), \"Master’s\")\n",
        "     .when(F.lower(F.col(edu_src)).contains(\"bachelor\"), \"Bachelor\")\n",
        "     .otherwise(\"Associate or Lower\")\n",
        ")\n",
        "\n",
        "# --- Optional: choose a hover occupation/title column if available\n",
        "occ_candidates = [\"OCCUPATION_NAME\", \"JOB_TITLE_NAME\", \"JOB_TITLE\", \"TITLE_RAW\",\n",
        "                  \"ONET_NAME\", \"OCC_TITLE\"]\n",
        "occ_col = next((c for c in occ_candidates if c in df.columns), None)\n",
        "\n",
        "# --- Build pandas dataframe for Plotly\n",
        "cols = [exp_col, sal_col, \"EDU_GROUP\"] + ([occ_col] if occ_col else [])\n",
        "pdf = (\n",
        "    df.select(*cols)\n",
        "      .where(col(sal_col).isNotNull() & (col(sal_col) > 0) & col(exp_col).isNotNull())\n",
        "      .toPandas()\n",
        ")\n",
        "\n",
        "# Jitter experience a bit to reduce overplotting\n",
        "rng = np.random.default_rng(42)\n",
        "pdf[\"exp_jitter\"] = pdf[exp_col].astype(float) + rng.uniform(-0.15, 0.15, size=len(pdf))\n",
        "\n",
        "# Rename for pretty axes\n",
        "pdf = pdf.rename(columns={sal_col: \"Average Salary (USD)\",\n",
        "                          \"exp_jitter\": \"Years of Experience\"})\n",
        "\n",
        "# --- Plot\n",
        "hover = [occ_col] if occ_col else None\n",
        "fig = px.scatter(\n",
        "    pdf,\n",
        "    x=\"Years of Experience\",\n",
        "    y=\"Average Salary (USD)\",\n",
        "    color=\"EDU_GROUP\",\n",
        "    hover_data=hover,\n",
        "    opacity=0.7,\n",
        "    title=\"Experience vs Salary by Education Level\"\n",
        ")\n",
        "\n",
        "fig.update_layout(\n",
        "    xaxis_title=\"Years of Experience\",\n",
        "    yaxis_title=\"Average Salary (USD)\",\n",
        "    legend_title=\"Education Group\",\n",
        "    width=1600, height=800,\n",
        "    margin=dict(l=40, r=40, t=80, b=120)\n",
        ")\n",
        "\n",
        "from IPython.display import HTML\n",
        "display(HTML(fig.to_html(include_plotlyjs=\"cdn\", full_html=False)))"
      ],
      "id": "f0913fbc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- **Explanation**: \"This chart illustrates that salaries tend to rise with years of experience across all education levels, though there's significant variability, particularly in early to mid-career stages. Higher education levels, like Master’s and PhD, generally exhibit higher salary clusters compared to Associate’s and Bachelor’s degrees, although there is some overlap at various experience stages.\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Salary by Remote Work Type\n",
        "\n",
        "- Split into three groups based on remote status: **Remote**, **Hybrid**, **Onsite** (includes blank/none).  \n",
        "- Plot a **scatter** of `MAX_YEARS_EXPERIENCE` (with jitter) vs `Average_Salary`, colored by remote group.  \n",
        "- Also create **salary histograms** for the three groups to compare distributions.  \n"
      ],
      "id": "b78a1d73"
    },
    {
      "cell_type": "code",
      "metadata": {
        "message": false
      },
      "source": [
        "#| echo: false\n",
        "#| warning: false\n",
        "#| fig-cap: Experience vs Salary by Remote Work Type\n",
        "\n",
        "from pyspark.sql import SparkSession, functions as F\n",
        "from pyspark.sql.types import DoubleType\n",
        "from pyspark.sql.functions import col\n",
        "from IPython.display import HTML, display\n",
        "import plotly.express as px\n",
        "import numpy as np\n",
        "\n",
        "# --- Spark / source df\n",
        "try:\n",
        "    spark\n",
        "except NameError:\n",
        "    spark = SparkSession.builder.appName(\"LightcastData\").getOrCreate()\n",
        "\n",
        "if any(t.name == \"job_postings_clean\" for t in spark.catalog.listTables()):\n",
        "    df = spark.table(\"job_postings_clean\")\n",
        "else:\n",
        "    df = (\n",
        "        spark.read\n",
        "        .option(\"header\", \"true\").option(\"inferSchema\", \"true\")\n",
        "        .option(\"multiLine\", \"true\").option(\"escape\", \"\\\"\")\n",
        "        .csv(\"data/lightcast_job_postings.csv\")\n",
        "    )\n",
        "\n",
        "# --- Experience column\n",
        "exp_candidates = [\"MAX_YEARS_EXPERIENCE\", \"YEARS_EXPERIENCE\",\n",
        "                  \"YEARS_OF_EXPERIENCE\", \"MIN_YEARS_EXPERIENCE\"]\n",
        "exp_col = next((c for c in exp_candidates if c in df.columns), None)\n",
        "if exp_col is None:\n",
        "    raise ValueError(f\"No experience column found. Tried: {exp_candidates}\")\n",
        "\n",
        "# --- Salary column (prefer Average_Salary)\n",
        "if \"Average_Salary\" in df.columns:\n",
        "    sal_col = \"Average_Salary\"\n",
        "else:\n",
        "    if \"SALARY_FROM\" in df.columns and \"SALARY_TO\" in df.columns:\n",
        "        df = df.withColumn(\n",
        "            \"Average_Salary\",\n",
        "            (F.col(\"SALARY_FROM\").cast(DoubleType()) + F.col(\"SALARY_TO\").cast(DoubleType())) / 2.0\n",
        "        )\n",
        "        sal_col = \"Average_Salary\"\n",
        "    elif \"SALARY\" in df.columns:\n",
        "        sal_col = \"SALARY\"\n",
        "    else:\n",
        "        raise ValueError(\"No salary column found (looked for Average_Salary, SALARY_FROM/TO, SALARY).\")\n",
        "\n",
        "df = df.withColumn(sal_col, col(sal_col).cast(DoubleType()))\n",
        "\n",
        "# --- Remote grouping\n",
        "# Use existing REMOTE_GROUP if present; otherwise derive from REMOTE_TYPE_NAME\n",
        "if \"REMOTE_GROUP\" in df.columns:\n",
        "    df = df.withColumn(\"REMOTE_GROUP\",\n",
        "        F.when(F.col(\"REMOTE_GROUP\").isin(\"Remote\", \"Hybrid\", \"Onsite\"), F.col(\"REMOTE_GROUP\"))\n",
        "         .otherwise(\"Onsite\")  # treat blanks/others as Onsite\n",
        "    )\n",
        "elif \"REMOTE_TYPE_NAME\" in df.columns:\n",
        "    df = df.withColumn(\"REMOTE_GROUP\",\n",
        "        F.when(F.col(\"REMOTE_TYPE_NAME\") == \"Remote\", \"Remote\")\n",
        "         .when(F.col(\"REMOTE_TYPE_NAME\") == \"Hybrid\", \"Hybrid\")\n",
        "         .otherwise(\"Onsite\")\n",
        "    )\n",
        "else:\n",
        "    # if no remote info exists, create a single group to avoid failure\n",
        "    df = df.withColumn(\"REMOTE_GROUP\", F.lit(\"Onsite\"))\n",
        "\n",
        "# --- Optional hover info\n",
        "hover_col = None\n",
        "for c in [\"JOB_TITLE_NAME\", \"TITLE_RAW\", \"OCCUPATION_NAME\", \"JOB_TITLE\"]:\n",
        "    if c in df.columns:\n",
        "        hover_col = c\n",
        "        break\n",
        "\n",
        "# --- Build pandas DF for plots\n",
        "cols = [exp_col, sal_col, \"REMOTE_GROUP\"] + ([hover_col] if hover_col else [])\n",
        "pdf = (\n",
        "    df.select(*cols)\n",
        "      .where(col(sal_col).isNotNull() & (col(sal_col) > 0) & col(exp_col).isNotNull())\n",
        "      .toPandas()\n",
        ")\n",
        "\n",
        "# jitter experience to reduce overplotting\n",
        "rng = np.random.default_rng(42)\n",
        "pdf[\"exp_jitter\"] = pdf[exp_col].astype(float) + rng.uniform(-0.15, 0.15, size=len(pdf))\n",
        "pdf = pdf.rename(columns={sal_col: \"Average Salary (USD)\", \"exp_jitter\": \"Years of Experience\"})\n",
        "\n",
        "# --- Scatter plot\n",
        "hover = [hover_col] if hover_col else None\n",
        "fig_scatter = px.scatter(\n",
        "    pdf,\n",
        "    x=\"Years of Experience\",\n",
        "    y=\"Average Salary (USD)\",\n",
        "    color=\"REMOTE_GROUP\",\n",
        "    hover_data=hover,\n",
        "    opacity=0.75,\n",
        "    title=\"Experience vs Salary by Remote Work Type\"\n",
        ")\n",
        "fig_scatter.update_layout(\n",
        "    legend_title=\"Remote Work Type\",\n",
        "    width=1600, height=800,\n",
        "    margin=dict(l=40, r=40, t=80, b=120)\n",
        ")\n",
        "\n",
        "# --- Histograms of salary by remote group (faceted)\n",
        "fig_hist = px.histogram(\n",
        "    pdf,\n",
        "    x=\"Average Salary (USD)\",\n",
        "    color=\"REMOTE_GROUP\",\n",
        "    barmode=\"overlay\",\n",
        "    opacity=0.6,\n",
        "    facet_col=\"REMOTE_GROUP\",\n",
        "    facet_col_spacing=0.06,\n",
        "    title=\"Salary Distribution by Remote Work Type\"\n",
        ")\n",
        "fig_hist.update_layout(\n",
        "    width=1600, height=500,\n",
        "    margin=dict(l=40, r=40, t=70, b=60),\n",
        "    legend_title=\"Remote Work Type\"\n",
        ")\n",
        "\n",
        "# --- Render only charts on the page\n",
        "display(HTML(fig_scatter.to_html(include_plotlyjs=\"cdn\", full_html=False)))\n",
        "display(HTML(fig_hist.to_html(include_plotlyjs=False, full_html=False)))"
      ],
      "id": "21df5028",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- **Explanation**: \"These charts reveal salary trends across onsite, remote, and hybrid roles. Most salaries cluster between $60k and $150k, but remote roles tend to lean towards the lower end, while onsite positions include more high-paying outliers. Hybrid roles follow a similar pattern. This indicates that remote work status isn’t the main factor influencing salary—job type and experience are likely more significant determinants.\"\n"
      ],
      "id": "7b737f16"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/usr/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}