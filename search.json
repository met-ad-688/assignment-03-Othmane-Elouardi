[
  {
    "objectID": "assignment03-othmane.html#loading-dataset-with-pyspark",
    "href": "assignment03-othmane.html#loading-dataset-with-pyspark",
    "title": "Assignment 3",
    "section": "1.1 Loading Dataset With PySpark",
    "text": "1.1 Loading Dataset With PySpark\n • Initialized a Spark session\n • Loaded the lightcast_data.csv file into a DataFrame\n • Displayed the schema and a sample of the dataset as a quick check\n# Imports\nimport pandas as pd\nimport plotly.express as px\nimport plotly.io as pio\nfrom pyspark.sql import SparkSession\nimport re\nimport numpy as np\nimport plotly.graph_objects as go\nfrom pyspark.sql.functions import col, split, explode, regexp_replace, transform, when\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.functions import col, monotonically_increasing_id\n# Spark session\nspark = (\n    SparkSession.builder\n    .appName(\"LightcastData\")\n    .getOrCreate()\n)\n\n# Load the CSV\ncsv_path = \"data/lightcast_job_postings.csv\"\n\ndf = (\n    spark.read\n        .option(\"header\", \"true\")\n        .option(\"inferSchema\", \"true\")\n        .option(\"multiLine\", \"true\")\n        .option(\"escape\", \"\\\"\")\n        .csv(csv_path)\n)\n\n# Temp view for SQL\ndf.createOrReplaceTempView(\"job_postings\")\n\n# Quick diagnostics \n#print(\"\\n--- Spark & Data Quick Check ---\")\n#print(\"Spark version:\", spark.version)\n#print(\"Row count (sampled below):\")\n#df.show(5, truncate=False)    \n# df.printSchema()"
  },
  {
    "objectID": "assignment03-othmane.html#data-cleaning",
    "href": "assignment03-othmane.html#data-cleaning",
    "title": "Assignment 3",
    "section": "1.2 DATA CLEANING",
    "text": "1.2 DATA CLEANING\nIn this section, we performed several cleaning steps to ensure the dataset is consistent and ready for analysis:\n\nCast salary/experience columns to numeric\n\nCompute medians (via approxQuantile) for imputation\n\nCreate Average_Salary with sensible fallbacks\n\nClean EDUCATION_LEVELS_NAME (remove newlines)\n\nDerive a simplified REMOTE_GROUP (Remote, Hybrid, Onsite)\n\n\n\n1.2.1 Casting\n# Cast target columns to numeric (DoubleType)\nfrom pyspark.sql.types import DoubleType\n\nnumeric_casts = {\n    \"SALARY_FROM\": DoubleType(),\n    \"SALARY_TO\":   DoubleType(),\n    \"SALARY\":      DoubleType(),\n    \"MIN_YEARS_EXPERIENCE\": DoubleType(),\n    \"MAX_YEARS_EXPERIENCE\": DoubleType()\n}\n\ndf = df.select(*[\n    F.col(c).cast(numeric_casts[c]).alias(c) if c in numeric_casts else F.col(c)\n    for c in df.columns\n])\n\n\n1.2.2 Medians\n# Helper to compute median with approxQuantile\ndef median_of(colname):\n    vals = df.filter(F.col(colname).isNotNull()) \\\n             .approxQuantile(colname, [0.5], 0.001)\n    return float(vals[0]) if vals else None\n\nmedian_from   = median_of(\"SALARY_FROM\")\nmedian_to     = median_of(\"SALARY_TO\")\nmedian_salary = median_of(\"SALARY\")\n\nprint(\"Medians:\", median_from, median_to, median_salary)\n\n1.2.2.1 Medians (output)\n\n\n\n\n  Medians\n  \n    SALARY_FROM: 88,000\n    SALARY_TO: 131,040\n    SALARY: 116,230\n  \n\n\n\n\n\n\n1.2.3 Average Salary\n# Choose a global fallback for missing salary values\nfallback = next(x for x in [median_salary, median_from, median_to] if x is not None)\n\ndf = df.withColumn(\n    \"Average_Salary\",\n    F.when(F.col(\"SALARY_FROM\").isNotNull() & F.col(\"SALARY_TO\").isNotNull(),\n           (F.col(\"SALARY_FROM\") + F.col(\"SALARY_TO\")) / 2.0\n    ).when(F.col(\"SALARY\").isNotNull(), F.col(\"SALARY\")\n    ).when(F.col(\"SALARY_FROM\").isNotNull(), F.col(\"SALARY_FROM\")\n    ).when(F.col(\"SALARY_TO\").isNotNull(), F.col(\"SALARY_TO\")\n    ).otherwise(F.lit(fallback))\n)\n\n# fill missing bounds for completeness\ndf = df.fillna({\"SALARY_FROM\": median_from, \"SALARY_TO\": median_to})\n\n1.2.3.1 Average Salary (output)\n\n\n\n    \n      Average Salary: 116,230 USD\n    \n    \n\n\n\n\n\n1.2.4 Clean Education Field\n# Remove \\r and \\n, then trim\ndf = df.withColumn(\n    \"EDUCATION_LEVELS_NAME\",\n    F.trim(\n        F.regexp_replace(\n            F.regexp_replace(F.col(\"EDUCATION_LEVELS_NAME\"), r\"\\r\", \" \"),\n            r\"\\n\", \" \"\n        )\n    )\n)\n\n1.2.4.1 Clean Education Field (output)\n\n\n\n      Education Field Summary (post-clean)\n      Total rows: 72,498 • Blank/Null: 44\n    \n    \n\n\n\nEducation Level\nCount\n\n\n\n\n[\\n \"Bachelor's degree\"\\n]\n29,969\n\n\n[\\n \"No Education Listed\"\\n]\n22,110\n\n\n[\\n \"Bachelor's degree\",\\n \"Master's degree\"\\n]\n10,501\n\n\n[\\n \"Associate degree\",\\n \"Bachelor's degree\"\\n]\n1,972\n\n\n[\\n \"Master's degree\"\\n]\n1,623\n\n\n[\\n \"High school or GED\"\\n]\n1,541\n\n\n[\\n \"High school or GED\",\\n \"Bachelor's degree\"\\n]\n1,408\n\n\n[\\n \"Bachelor's degree\",\\n \"Master's degree\",\\n \"Ph.D. or professional degree\"\\n]\n853\n\n\n[\\n \"Associate degree\"\\n]\n786\n\n\n[\\n \"Associate degree\",\\n \"Bachelor's degree\",\\n \"Master's degree\"\\n]\n267\n\n\n\n\n\n\n\n\n1.2.5 Remote Group\n# Normalize to Remote / Hybrid / Onsite\ndf = df.withColumn(\n    \"REMOTE_GROUP\",\n    F.when(F.col(\"REMOTE_TYPE_NAME\") == \"Remote\", \"Remote\")\n     .when(F.col(\"REMOTE_TYPE_NAME\") == \"Hybrid\", \"Hybrid\")\n     .otherwise(\"Onsite\")\n)\n\n\n1.2.6 Final Check\ndf.createOrReplaceTempView(\"job_postings_clean\")\n\nprint(\"Rows retained:\", df.count())\n\ndf.select(\"Average_Salary\",\"SALARY\",\"EDUCATION_LEVELS_NAME\",\n          \"REMOTE_TYPE_NAME\",\"REMOTE_GROUP\",\n          \"MIN_YEARS_EXPERIENCE\",\"MAX_YEARS_EXPERIENCE\") \\\n  .show(5, truncate=False)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "Welcome to Assignment 3\nThis site contains the full report and analysis for Assignment 3.\nUse the navigation bar to explore the report and results."
  }
]